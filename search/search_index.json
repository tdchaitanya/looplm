{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LoopLM - Loop Language Models in your Terminal","text":"<p>\ud83d\udeab\ud83d\udeab\ud83d\udeab This tool is still in active development</p> <p><code>looplm</code> -- LoopLM is a highly customisable command-line interface that seamlessly integrates various Language Models into your development workflow. Whether you need quick code assistance, want to explore ideas, or engage in interactive conversations with LLMs, LoopLM provides an intuitive terminal-based interface to access state-of-the-art language models.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>\ud83d\ude80 Support for multiple LLM providers: Works with OpenAI, Anthropic, Google Gemini, Azure OpenAI, AWS Bedrock, and other providers through LiteLLM integration. You can easily switch between different providers and models</li> <li>\ud83d\udcc2 File Integration: Include files directly in your prompts using @file directives, supporting code files, logs, configurations, and even PDFs and documents</li> <li>\ud83d\udd12 Secure Configuration: All API keys and credentials are stored securely using encryption</li> <li>\ud83d\udcbb Simple CLI: Intuitive command-line interface for quick access to AI capabilities</li> <li>\ud83d\udcac Interactive Chat Mode: Engage in persistent, interactive conversations with your preferred LLM using looplm chat</li> <li>\ud83d\udd0d Rich Output: Beautiful terminal output with markdown support</li> <li>\ud83d\udd0d Smart Context: Maintain conversation context and system prompts for consistent interactions</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<ol> <li> <p>Install LoopLM (pipx is recommended): <pre><code>pipx install looplm\n</code></pre></p> </li> <li> <p>Configure your first provider: <pre><code>looplm --configure\n</code></pre></p> </li> <li> <p>Start using the CLI with direct file support: <pre><code># Review code with file directive\nlooplm \"Review this code: @file(src/main.py)\"\n\n# Compare implementations\nlooplm \"Compare these files: @file(v1.py) vs @file(v2.py)\"\n\n# Analyze logs\nlooplm \"Check this log: @file(/var/log/app.log)\"\n</code></pre></p> </li> <li> <p>Start an interactive chat session: <pre><code>looplm chat\n</code></pre></p> </li> </ol>"},{"location":"#why-looplm","title":"Why LoopLM?","text":"<p>LoopLM is designed for developers who: - Want quick access to LLMs without leaving the terminal - Need to frequently analyze code, logs, and configuration files - Work with multiple LLM providers and need a unified interface - Want to integrate LLM assistance into their development workflow - Need to maintain context across development sessions</p>"},{"location":"#requirementss","title":"Requirementss","text":"<ul> <li>Python 3.10 or higher</li> <li>API keys for the providers you want to use</li> </ul> <p>For detailed usage instructions, see: - Direct Usage Guide - Chat Mode Guide - Configuration Guide</p>"},{"location":"chat-usage/","title":"Chat Mode Usage","text":"<p>Chat mode in LoopLM enables interactive, persistent conversations with language models. This guide covers how to effectively use chat mode for your development workflow.</p>"},{"location":"chat-usage/#starting-a-chat-session","title":"Starting a Chat Session","text":""},{"location":"chat-usage/#basic-start","title":"Basic Start","text":"<pre><code># Start with default provider\nlooplm chat\n\n# Use specific provider\nlooplm chat --provider anthropic\n\n# Use specific model\nlooplm chat --provider openai --model gpt-4o-mini\n</code></pre>"},{"location":"chat-usage/#core-features","title":"Core Features","text":""},{"location":"chat-usage/#session-management","title":"Session Management","text":"<pre><code># Save current session\n/save\nEnter session name: project-planning\n\n# List available sessions\n/list\n\n# Load a previous session\n/load\n\n# Start a new session\n/new\n\n# Rename current session\n/rename\n\n# Delete a session\n/delete\n</code></pre>"},{"location":"chat-usage/#environment-control","title":"Environment Control","text":"<pre><code># Clear chat history\n/clear\n\n# Change model\n/model\n\n# View token usage\n/usage\n\n# Exit chat mode\n/quit\n</code></pre>"},{"location":"chat-usage/#system-prompt-management","title":"System Prompt Management","text":"<p>System prompts allow you to give a role to the langiage model, for more details see here <pre><code># View/modify system prompt\n/system\n\n# Available options:\n1. Use saved prompt\n2. Create new prompt\n3. Save current prompt\n4. Delete saved prompt\n</code></pre></p>"},{"location":"chat-usage/#common-use-cases","title":"Common Use Cases","text":""},{"location":"chat-usage/#interactive-development","title":"Interactive Development","text":"<pre><code>&gt; Help me design a Python class for handling HTTP requests\n\n&gt; Can you add error handling to that class?\n\n&gt; Now let's write unit tests for it\n</code></pre>"},{"location":"chat-usage/#code-review-sessions","title":"Code Review Sessions","text":"<pre><code>&gt; I want to review this code:\n[paste code]\n\n&gt; What potential issues do you see?\n\n&gt; How can we improve the error handling?\n</code></pre>"},{"location":"chat-usage/#learning-sessions","title":"Learning Sessions","text":"<pre><code>&gt; Explain how Python's async/await works\n\n&gt; Can you give me an example?\n\n&gt; What are some common pitfalls to avoid?\n</code></pre>"},{"location":"chat-usage/#project-planning","title":"Project Planning","text":"<pre><code>&gt; Let's design a system architecture for a web app\n\n&gt; What database would you recommend for this use case?\n\n&gt; Can you help me break this down into smaller tasks?\n</code></pre>"},{"location":"chat-usage/#file-integration","title":"File Integration","text":"<p>LoopLM supports including file contents directly in your chat messages using the @file directive:</p>"},{"location":"chat-usage/#syntax-options","title":"Syntax Options","text":"<pre><code># Three ways to include files:\n\n# 1. Quoted path\n&gt; Let's review this code: @file(\"src/main.py\")\n\n# 2. Space-separated path\n&gt; Check this configuration: @file config/settings.yml\n\n# 3. Using absolute paths\n&gt; Analyze this log: @file(logs/error.log)\n</code></pre>"},{"location":"chat-usage/#use-cases-with-files","title":"Use Cases with Files","text":""},{"location":"chat-usage/#code-review-sessions_1","title":"Code Review Sessions","text":"<pre><code>&gt; I want to review our authentication module: @file(\"src/auth/auth.py\")\n\n&gt; Now let's look at its tests: @file(\"tests/auth/test_auth.py\")\n\n&gt; Can we compare it with the new implementation? @file(\"src/auth/auth_v2.py\")\n</code></pre>"},{"location":"chat-usage/#configuration-review","title":"Configuration Review","text":"<pre><code>&gt; Please review these configurations:\n\n&gt; Here's our production config: @file(\"config/prod.yml\")\n&gt; And staging config: @file(\"config/staging.yml\")\n\n&gt; What are the key differences?\n</code></pre>"},{"location":"chat-usage/#log-analysis","title":"Log Analysis","text":"<pre><code>&gt; Can you help me understand this error?\n&gt; @file(/var/log/app/error.log)\n\n&gt; And here's the related configuration: @file(\"config/logging.yml\")\n</code></pre>"},{"location":"chat-usage/#documentation-work","title":"Documentation Work","text":"<pre><code>&gt; I need to document this module:\n&gt; @file(\"src/core/module.py\")\n\n&gt; It uses these utilities: @file(\"src/utils/helpers.py\")\n\n&gt; Can you help me write comprehensive documentation?\n</code></pre>"},{"location":"chat-usage/#workflow-examples","title":"Workflow Examples","text":""},{"location":"chat-usage/#development-session","title":"Development Session","text":"<pre><code># Start a development session\nlooplm chat --provider anthropic\n&gt; I need to build a REST API with FastAPI\n\n# Save the session\n/save\nEnter session name: fastapi-development\n\n# Later, resume the session\nlooplm chat\n/load\nSelect session: fastapi-development\n</code></pre>"},{"location":"chat-usage/#code-review-workflow","title":"Code Review Workflow","text":"<pre><code># Start a code review session\nlooplm chat --provider openai --model gpt-4\n&gt; I'm going to share some code for review\n\n# After discussion\n/save\nEnter session name: code-review-sprint-1\n\n# Share findings with team\n/usage  # Check token usage for reporting\n</code></pre>"},{"location":"chat-usage/#learning-session","title":"Learning Session","text":"<pre><code># Start a learning session\nlooplm chat\n&gt; I want to learn about design patterns\n\n# Save progress\n/save\nEnter session name: design-patterns-study\n\n# Change model for deeper insights\n/model\nSelect: anthropic/claude-3-opus\n\n# Continue learning\n&gt; Can you explain the Factory pattern?\n</code></pre>"},{"location":"chat-usage/#best-practices","title":"Best Practices","text":""},{"location":"chat-usage/#session-organization","title":"Session Organization","text":"<ol> <li> <p>Use Descriptive Names <pre><code>project-backend-design\napi-security-review\npython-concurrency-learning\n</code></pre></p> </li> <li> <p>Regular Saving</p> <ul> <li>Save after significant insights</li> <li>Save before changing topics</li> <li>Save before testing different approaches</li> </ul> </li> <li> <p>Clean Session Management <pre><code>/list  # Review available sessions\n/delete  # Remove obsolete sessions\n/new  # Start fresh for new topics\n</code></pre></p> </li> </ol>"},{"location":"configuration/","title":"Configuration Guide","text":"<p>LoopLM provides a straightforward way to configure different LLM providers. All configuration is handled through the CLI, with credentials stored securely using encryption.</p>"},{"location":"configuration/#initial-setup","title":"Initial Setup","text":"<p>To start configuring LoopLM, run:</p> <pre><code>looplm --configure\n</code></pre> <p>This will launch an interactive setup process where you can configure one or more providers.</p>"},{"location":"configuration/#supported-providers","title":"Supported Providers","text":"<p>LoopLM supports the following providers:</p>"},{"location":"configuration/#anthropic","title":"Anthropic","text":"<p>Required environment variables: - <code>ANTHROPIC_API_KEY</code></p> <p>Example model: <code>claude-3-5-sonnet-20240620</code></p>"},{"location":"configuration/#openai","title":"OpenAI","text":"<p>Required environment variables: - <code>OPENAI_API_KEY</code></p> <p>Example model: <code>gpt-4o</code></p>"},{"location":"configuration/#google-gemini","title":"Google Gemini","text":"<p>Required environment variables: - <code>GEMINI_API_KEY</code></p> <p>Example model: <code>gemini/gemini-pro</code></p>"},{"location":"configuration/#azure-openai","title":"Azure OpenAI","text":"<p>Required environment variables: - <code>AZURE_API_KEY</code> - <code>AZURE_API_BASE</code> - <code>AZURE_API_VERSION</code></p> <p>Example model: <code>azure/gpt-4o</code></p>"},{"location":"configuration/#aws-bedrock","title":"AWS Bedrock","text":"<p>Required environment variables: - <code>AWS_ACCESS_KEY_ID</code> - <code>AWS_SECRET_ACCESS_KEY</code> - <code>AWS_REGION_NAME</code></p> <p>Example model: <code>anthropic.claude-3-5-sonnet-20240620-v1:0</code></p>"},{"location":"configuration/#other-providers","title":"Other Providers","text":"<p>LoopLM supports any provider that's compatible with LiteLLM. When configuring other providers, you'll need to:</p> <ol> <li>Specify the provider name</li> <li>Enter the required environment variables</li> <li>Specify the model name according to LiteLLM's documentation</li> </ol>"},{"location":"configuration/#configuration-commands","title":"Configuration Commands","text":""},{"location":"configuration/#view-current-configuration","title":"View Current Configuration","text":"<pre><code>looplm --status\n</code></pre> <p>This shows: - Configured providers - Default provider and model - Provider status</p>"},{"location":"configuration/#set-default-provider","title":"Set Default Provider","text":"<pre><code>looplm --set-default &lt;provider&gt;\n</code></pre> <p>Example: <pre><code>looplm --set-default anthropic\n</code></pre></p>"},{"location":"configuration/#reset-configuration","title":"Reset Configuration","text":"<p>Reset all configuration: <pre><code>looplm --reset\n</code></pre></p> <p>Reset specific provider: <pre><code>looplm --reset-provider anthropic\n</code></pre></p>"},{"location":"configuration/#using-different-providers","title":"Using Different Providers","text":""},{"location":"configuration/#in-direct-prompts","title":"In Direct Prompts","text":"<pre><code># Use specific provider\nlooplm --provider anthropic \"Explain quantum computing\"\n\n# Use specific model\nlooplm --provider openai --model gpt-4 \"Write a regex pattern\"\n</code></pre>"},{"location":"configuration/#in-chat-mode","title":"In Chat Mode","text":"<pre><code># Start chat with specific provider\nlooplm chat --provider anthropic\n\n# Start chat with specific model\nlooplm chat --provider openai --model gpt-4o-mini\n</code></pre>"},{"location":"configuration/#configuration-storage","title":"Configuration Storage","text":"<p>LoopLM stores configuration in two locations in your home directory:</p> <ol> <li><code>.looplm/config.json</code>: General configuration (non-sensitive)</li> <li><code>.looplm/secrets.enc</code>: Encrypted API keys and credentials</li> <li><code>.looplm/sessions/</code>: Saved chat sessions</li> <li><code>.looplm/prompts/</code>: System prompts</li> </ol> <p>The configuration is encrypted using Fernet symmetric encryption, ensuring your API keys remain secure.</p>"},{"location":"configuration/#additional-environment-variables","title":"Additional Environment Variables","text":"<p>When configuring a provider, you can set additional environment variables that might be required for your specific use case. These will be stored securely with your other credentials.</p>"},{"location":"configuration/#managing-system-prompts","title":"Managing System Prompts","text":"<p>In chat mode, you can manage system prompts that define the assistant's behavior:</p> <ol> <li>View current prompt: <code>/system</code></li> <li>Create new prompt: Use the system command menu</li> <li>Save prompts for reuse: Save option in system menu</li> <li>Load saved prompts: Load option in system menu</li> </ol> <p>This allows you to customize the assistant's behavior for different use cases.</p>"},{"location":"direct-usage/","title":"Direct Command Usage","text":"<p>This guide covers using LoopLM for quick, one-off interactions with language models directly from your command line.</p>"},{"location":"direct-usage/#basic-usage","title":"Basic Usage","text":""},{"location":"direct-usage/#simple-prompts","title":"Simple Prompts","text":"<pre><code># Basic query\nlooplm \"Write a Python function to calculate factorial\"\n</code></pre>"},{"location":"direct-usage/#provider-selection","title":"Provider Selection","text":"<pre><code># Use specific provider\nlooplm --provider gemini \"Explain async/await in Python\"\n\n# Use specific model\nlooplm --provider openai --model gpt-4o-mini \"Write unit tests for a login function\"\n</code></pre>"},{"location":"direct-usage/#input-methods","title":"Input Methods","text":""},{"location":"direct-usage/#file-input","title":"File Input","text":"<p>LoopLM supports several ways to include file contents in your prompts:</p> <ol> <li> <p>Using @file directive: <pre><code># Using quoted path\nlooplm \"Explain this code: @file(\\\"src/main.py\\\")\"\n\n# Using space-separated path\nlooplm \"Review this configuration: @file config/settings.yml\"\n\n# Using absolute paths\nlooplm \"Analyze this log: @file(/var/log/app.log)\"\n</code></pre></p> </li> <li> <p>Using command substitution (traditional method): <pre><code># Pass file content\nlooplm \"Explain this code: $(cat script.py)\"\n\n# Multiple files\nlooplm \"Compare these implementations: $(cat impl1.py) vs $(cat impl2.py)\"\n</code></pre></p> </li> </ol> <p>The @file directive supports: - Text files (code, logs, config files, etc.) - Common document formats (PDF, Word, Excel, etc.) through automatic conversion - Both relative and absolute paths - Multiple file inclusions in a single prompt</p>"},{"location":"direct-usage/#pipe-input","title":"Pipe Input","text":"<pre><code># Pipe error output\npython script.py 2&gt;&amp;1 | looplm \"Help me debug this error\"\n\n# Pipe file content\ncat complex_code.py | looplm \"Explain this code in detail\"\n\n# Process command output\ngit diff | looplm \"Summarize these changes\"\n</code></pre>"},{"location":"direct-usage/#common-use-cases","title":"Common Use Cases","text":""},{"location":"direct-usage/#code-tasks","title":"Code Tasks","text":"<pre><code># Code review with file directive\nlooplm \"Review this implementation: @file(src/auth.py)\"\n\n# Documentation with multiple files\nlooplm \"Write documentation for this module: @file(src/module.py) and its tests @file(tests/test_module.py)\"\n\n# Bug fixing\nlooplm \"Fix this buggy code: @file(src/buggy.py)\"\n\n# Code analysis\nlooplm \"Analyze the complexity of this function: @file(src/complex_function.py)\"\n</code></pre>"},{"location":"direct-usage/#development-support","title":"Development Support","text":"<pre><code># Git commit messages\ngit diff --cached | looplm \"Write a commit message for these changes\"\n\n# API documentation\ncat api_endpoint.py | looplm \"Write OpenAPI documentation for this endpoint\"\n\n# Config file generation\nlooplm \"Create a Docker Compose file for a Python web app with Redis and PostgreSQL\"\n</code></pre>"},{"location":"direct-usage/#learning-and-understanding","title":"Learning and Understanding","text":"<pre><code># Concept explanation\nlooplm \"Explain how Python's GIL works\"\n\n# Code breakdown\ncat complex_algorithm.py | looplm \"Break down how this algorithm works\"\n\n# Best practices\nlooplm \"What are the best practices for Python error handling?\"\n</code></pre>"},{"location":"direct-usage/#tips-and-tricks","title":"Tips and Tricks","text":""},{"location":"direct-usage/#shell-aliases","title":"Shell Aliases","text":"<p>Add these to your <code>.bashrc</code> or <code>.zshrc</code>:</p> <pre><code># Quick code explanation\nalias explain='looplm \"Explain this code: \"'\n\n# Debug helper\nalias debug='looplm \"Help debug this error: \"'\n\n# Documentation generator\nalias docstring='looplm \"Write a docstring for: \"'\n\n# Git commit helper\nalias commit-msg='git diff --cached | looplm \"Write a commit message\"'\n</code></pre>"},{"location":"direct-usage/#shell-functions","title":"Shell Functions","text":"<pre><code># Function to explain the last error\nexplain_error() {\n    local error_output=$(fc -ln -1 2&gt;&amp;1)\n    looplm \"Explain this error and suggest fixes: $error_output\"\n}\n\n# Function to document Python functions\ndocument_function() {\n    if [ -z \"$1\" ]; then\n        echo \"Usage: document_function &lt;python_file&gt;\"\n        return 1\n    fi\n    cat \"$1\" | looplm \"Write comprehensive docstrings for this code\"\n}\n</code></pre>"},{"location":"direct-usage/#integration-with-development-tools","title":"Integration with Development Tools","text":""},{"location":"direct-usage/#git-hooks","title":"Git Hooks","text":"<pre><code>#!/bin/bash\n# .git/hooks/pre-commit\n# Auto-generate commit message based on staged changes\n\nstaged_diff=$(git diff --cached)\nif [ -n \"$staged_diff\" ]; then\n    echo \"$staged_diff\" | looplm \"Write a concise commit message\" &gt; .git/COMMIT_EDITMSG\nfi\n</code></pre>"},{"location":"direct-usage/#editor-integration-vscode-tasksjson","title":"Editor Integration (VSCode tasks.json)","text":"<pre><code>{\n    \"version\": \"2.0.0\",\n    \"tasks\": [\n        {\n            \"label\": \"Document Function\",\n            \"type\": \"shell\",\n            \"command\": \"cat ${file} | looplm 'Write a docstring for this function, just return the docstring and nothing else'\",\n            \"presentation\": {\n                \"reveal\": \"always\",\n                \"panel\": \"new\"\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"direct-usage/#best-practices","title":"Best Practices","text":"<ol> <li>Handle Large Inputs <pre><code># For large files, focus on specific sections\ncat large_file.py | grep -A 10 \"class MyClass\" | looplm \"Explain this class\"\n</code></pre></li> <li>Use Appropriate Models <pre><code># Complex tasks: Use more capable models\nlooplm --provider anthropic --model claude-3.5-sonnet \"Design a system architecture\"\n\n# Simple tasks: Use faster models\nlooplm --provider openai --model gpt-4o-mini \"Format this JSON\"\n</code></pre></li> <li>Provide Context When Needed <pre><code># Include relevant details\nlooplm \"Write a unit test for this function considering these edge cases: $(cat edge_cases.txt)\"\n</code></pre></li> <li>Keep Prompts Clear and Specific <pre><code># Good\nlooplm \"Write a Python function to sort a list of dictionaries by the 'date' key\"\n\n# Less effective\nlooplm \"Sort dictionaries\"\n</code></pre></li> </ol>"}]}